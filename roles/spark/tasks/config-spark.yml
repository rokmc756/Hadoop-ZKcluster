---
- name: Create Link to Spark Directory
  file:
    src: "{{ spark_root_dir }}"
    dest: "{{ _spark.base_path }}/spark"
    owner: "{{ _hadoop.user }}"
    group: "{{ _hadoop.group }}"
    state: link


- name: Copy Spark Slaves
  template: src={{ item }}.j2 dest={{ _hadoop.base_path }}/{{ spark_file_name }}/conf/{{ item }} mode=644 owner={{ _spark.user }} group={{ _spark.group }}
  with_items:
    - slaves
    - workers


- name: Copy Spark Config File
  copy:
    src: "{{ _hadoop.base_path }}/{{ spark_file_name }}/conf/{{ item }}.template"
    dest: "{{ _hadoop.base_path }}/{{ spark_file_name }}/conf/{{ item }}"
    owner: "{{ _spark.user }}"
    group: "{{ _spark.group }}"
    mode: '0644'
    remote_src: yes
  with_items:
    - "spark-defaults.conf"


- name: Add Hadoop Master Metrics into Ganglia
  become_user: "{{ _spark.user }}"
  lineinfile:
    dest: "{{ _hadoop.base_path }}/{{ spark_file_name }}/conf/{{ item.file }}"
    line: '{{ item.line }}'
  with_items:
    - { file: "spark-defaults.conf", line: "hdfs://localhost:9000/user/hive/warehouse" }


- name: Copy Spark Env Script
  template: src=spark-env.sh.j2 dest=/etc/profile.d/spark-env.sh mode=755 owner=root group=root


- name: Set Spark Env
  shell: source /etc/profile.d/spark-env.sh


- name: Create Spark Events Directory
  file:
    path: "/tmp/spark-events"
    state: directory
    owner: root
    group: root
    mode: 0755


#- name: Deploy the Customized Spark log4j Properties
#  template:
#    src: "{{ log4j_template_file }}.j2"
#    dest: "{{ _spark.base_path }}/spark-{{ spark_version }}/conf/{{ log4j_template_file }}"
#    owner: "{{ _spark.user }}"
#    group: "{{ _spark.user }}"

