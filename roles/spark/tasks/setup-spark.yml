---
#- name: Deploy hive-site.xml in spark's conf directory
#  template:
#    src: "templates/hive-site.xml.j2"
#    dest: "{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/hive-site.xml"
#    force: yes
#  when:
#   - spark_hive_metastore_db_installed
#   - spark_hive_site_properties|length > 0


#- name: Ensure hive-site.xml removed from spark's conf directory
#  file:
#    path: "{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/hive-site.xml"
#    state: absent
#  when:
# - not spark_hive_metastore_db_installed


- name: Ensure that hive-site.xml Should be Removed from Spark's Conf Directory
  file:
    path: "{{ _spark.base_path }}/{{ spark_file_name }}.{{ _spark.bin_type }}/conf/hive-site.xml"
    state: absent
#  when:
# - not spark_hive_metastore_db_installed


- name: Check if Apache Spark Tarball File Already Exists
  stat: path={{ _spark.base_path }}/{{ spark_file_name }}.{{ _spark.bin_type }}
  register: check_spark_tarball
- debug: msg={{ check_spark_tarball }}


- name: Check if Apache Spark Installation Directory Already Exists
  stat: path={{ _spark.base_path }}/{{ spark_file_name }}
  register: check_spark_install_dir


- name: Sanity Check for Existence of {{ spark_file_name }}.{{ _spark.bin_type }} file or {{ spark_file_name }} Directory
  debug: msg="Both {{ spark_file_name }}.{{ _spark.bin_type }} file and {{ spark_file_name }} Directory Already Exists"
  changed_when: check_spark_tarball.stat.exists == True and check_spark_install_dir.stat.exists == True


- name: Download Spark Software Binaries
  get_url:
    url: "{{ item.url }}"
    dest: "{{ item.dest }}"
  with_items:
    - { url: "{{ _spark.download_url }}/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop{{ _hadoop.major_version }}.tgz",
        dest: "{{ _spark.base_path }}/spark-{{ spark_version }}-bin-hadoop{{ _hadoop.major_version }}.tgz" }
  when: _spark.download == true and check_spark_tarball.stat.exists != True


- name: Copy Apache Spark Tarball from local directory, roles/spark/files
  copy: src={{ spark_file_name }}.{{ _spark.bin_type }} dest={{ _spark.base_path }}/{{ spark_file_name }}.{{ _spark.bin_type }} mode=0644 owner={{ _hadoop.user }} group={{ _hadoop.group }}
  register: local_copy_dss
  when: _spark.download == false and check_spark_tarball.stat.exists != True


- name: Unarchive Apache Spark Tarball
  unarchive:
    src: "{{ _spark.base_path }}/{{ spark_file_name }}.{{ _spark.bin_type }}"
    dest: "{{ _spark.base_path }}"
    owner: "{{ _spark.user }}"
    group: "{{ _spark.group }}"
    mode: "0755"
    remote_src: yes
  register: unarchive_spark_tarball
  when: check_spark_install_dir.stat.exists == false


- name: Change Permission of Apache Spark Directory
  file:
    path: "{{ item.dir }}"
    state: "{{ item.state }}"
    mode: "{{ item.perm }}"
    owner: "{{ item.owner }}"
    group: "{{ item.group }}"
    recurse: "{{ item.recurse }}"
  with_items:
    - { state: "directory", perm: "0755", recurse: "yes", dir: "{{ spark_root_dir }}", owner: "{{ _hbase.user }}", group: "{{ _hbase.group }}" }


- name: Install PySpark Pip Module
  become_user: "{{ _spark.user }}"
  shell: |
    pip install pyspark
  register: start_pyspark_pip
  args:
    executable: /bin/bash
    chdir: "{{ _spark.base_path }}"
  environment:
    PATH: "{{ spark_root_dir }}/sbin:{{ spark_root_dir }}/bin:/usr/local/bin:{{ ansible_env.PATH }}"

