#- name: Unzip Spark
#  shell: "tar -xvf {{ download_path }}/spark-{{ spark_version }}-bin-hadoop{{ hadoop_version }}.tgz -C {{ spark_path }} --strip-components=1"

#- name: Deploy hive-site.xml in spark's conf directory
#  template:
#    src: "templates/hive-site.xml.j2"
#    dest: "{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/hive-site.xml"
#    force: yes
#  when:
#   - spark_hive_metastore_db_installed
#   - spark_hive_site_properties|length > 0


#- name: Ensure hive-site.xml removed from spark's conf directory
#  file:
#    path: "{{ spark_usr_parent_dir }}/spark-{{ spark_version }}/conf/hive-site.xml"
#    state: absent
#  when:
# - not spark_hive_metastore_db_installed


# - name: Create Link to Spark Directory
#  become: yes
#  file: src={{ _spark.base_path }}/{{ spark_file_name }} dest={{ _spark.baseh_path }}/spark state=link owner={{ _spark.user }} group={{ _spark.user }}


#- name: Configure Spark to Run in Standalone Deploy Mode
#  become: yes
#  template: src=spark-env.sh.j2 dest={{ _spark.base_path }}/{{ spark_file_name }}/conf/spark-env.sh mode="u=rwx,g=rx,o=rx"
#  tags: config_spark


###################################################################################################
# Need to check if it needs
###################################################################################################
# - name: Set Spark Distribution Fact
#  set_fact: spark_path=spark-{{ spark_version }}-bin-hadoop{{ _hadoop.major_version }}
#
#- name: Check Spark Downloaded
#  local_action: >
#    command test -f {{ _spark.temp_dir }}/{{ spark_path }}.tgz
#  register: spark_downloaded
#  failed_when: spark_downloaded.rc not in [0, 1]
#  changed_when: False
#  run_once: true
#
#- name: Download Spark
#  local_action: get_url url="{{ _spark.mirror }}/{{ spark_path }}.tgz" dest="{{ _spark.temp_dir }}"
#  when: spark_downloaded.rc == 1
#  run_once: true
###################################################################################################



- name: Ensure that hive-site.xml Should be Removed from Spark's Conf Directory
  file:
    path: "{{ _spark.base_path }}/{{ spark_file_name }}.{{ _spark.bin_type }}/conf/hive-site.xml"
    state: absent
#  when:
# - not spark_hive_metastore_db_installed


- name: Check if Apache Spark Tarball File Already Exists
  stat: path={{ _spark.base_path }}/{{ spark_file_name }}.{{ _spark.bin_type }}
  register: check_spark_tarball
- debug: msg={{ check_spark_tarball }}


- name: Check if Apache Spark Installation Directory Already Exists
  stat: path={{ _spark.base_path }}/{{ spark_file_name }}
  register: check_spark_install_dir


- name: Sanity check for existence of {{ spark_file_name }}.{{ spark.bin_type }} file or {{ spark_file_name }} directory
  debug: msg="Both {{ spark_file_name }}.{{ spark.bin_type }} file and {{ spark_file_name }} Directory Already Exists"
  changed_when: check_spark_tarball.stat.exists == True and check_spark_install_dir.stat.exists == True


- name: Download Spark Software Binaries
  get_url:
    url: "{{ item.url }}"
    dest: "{{ item.dest }}"
  with_items:
    - { url: "{{ _spark.download_url }}/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop{{ _hadoop.major_version }}.tgz",
        dest: "{{ _spark.base_path }}/spark-{{ spark_version }}-bin-hadoop{{ _hadoop.major_version }}.tgz" }
  when: _spark.download == true and check_spark_tarball.stat.exists != True


- name: Copy Apache Spark Tarball from local directory, roles/spark/files
  copy: src={{ spark_file_name }}.{{ _spark.bin_type }} dest={{ _spark.base_path }}/{{ spark_file_name }}.{{ _spark.bin_type }} mode=0644 owner={{ _hadoop.user }} group={{ _hadoop.group }}
  register: local_copy_dss
  when: _spark.download == false and check_spark_tarball.stat.exists != True


- name: Unarchive Apache Spark Tarball
  unarchive:
    src: "{{ _spark.base_path }}/{{ spark_file_name }}.{{ _spark.bin_type }}"
    dest: "{{ _spark.base_path }}"
    owner: "{{ _spark.user }}"
    group: "{{ _spark.group }}"
    mode: "0755"
    remote_src: yes
  register: unarchive_spark_tarball
  when: check_spark_install_dir.stat.exists == false


- name: Install PySpark Pip Module
  shell: |
    su {{ _hadoop.user }} -c 'pip install pyspark'

