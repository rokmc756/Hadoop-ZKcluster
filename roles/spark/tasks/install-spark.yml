#- name: Unzip Spark
#  shell: "tar -xvf {{ download_path }}/spark-{{ spark_version }}-bin-hadoop{{ hadoop_version }}.tgz -C {{ spark_path }} --strip-components=1"

#- name: Deploy hive-site.xml in spark's conf directory
#  template:
#    src: "templates/hive-site.xml.j2"
#    dest: "{{spark_usr_parent_dir}}/spark-{{spark_version}}/conf/hive-site.xml"
#    force: yes
#  when:
#   - spark_hive_metastore_db_installed
#   - spark_hive_site_properties|length > 0


#- name: Ensure hive-site.xml removed from spark's conf directory
#  file:
#    path: "{{spark_usr_parent_dir}}/spark-{{spark_version}}/conf/hive-site.xml"
#    state: absent
#  when:
# - not spark_hive_metastore_db_installed


# - name: Create Link to Spark Directory
#  become: yes
#  file: src={{ spark.base_path }}/{{ spark_file_name }} dest={{ spark.baseh_path }}/spark state=link owner={{ spark.user }} group={{ spark.user }}


#- name: Configure Spark to Run in Standalone Deploy Mode
#  become: yes
#  template: src=spark-env.sh.j2 dest={{ spark.base_path }}/{{ spark_file_name }}/conf/spark-env.sh mode="u=rwx,g=rx,o=rx"
#  tags: config_spark


###################################################################################################
# Need to check if it needs
###################################################################################################
# - name: Set Spark Distribution Fact
#  set_fact: spark_path=spark-{{ spark_version }}-bin-hadoop{{ hadoop.major_version }}
#
#- name: Check Spark Downloaded
#  local_action: >
#    command test -f {{ spark.temp_dir }}/{{ spark_path }}.tgz
#  register: spark_downloaded
#  failed_when: spark_downloaded.rc not in [0, 1]
#  changed_when: False
#  run_once: true
#
#- name: Download Spark
#  local_action: get_url url="{{ spark.mirror }}/{{ spark_path }}.tgz" dest="{{ spark.temp_dir }}"
#  when: spark_downloaded.rc == 1
#  run_once: true
###################################################################################################



- name: Ensure that hive-site.xml Should be Removed from Spark's Conf Directory
  file:
    path: "{{ spark.base_path }}/{{ spark_file_name }}.{{ spark.bin_type }}/conf/hive-site.xml"
    state: absent
#  when:
# - not spark_hive_metastore_db_installed


- name: Check if Apache Spark Tarball File Already Exists
  stat: path={{ spark.base_path }}/{{ spark_file_name }}.{{ spark.bin_type }}
  register: spark_tarball_exists

# - debug: msg={{ spark_tarball_exists }}


- name: Check if Apache Spark Installation Directory Already Exists
  stat: path={{ spark.base_path }}/{{ spark_file_name }}
  register: spark_install_dir_exists


- name: Sanity check for existence of {{ spark_file_name }}.{{ spark.bin_type }} file or {{ spark_file_name }} directory
  debug: msg="Both {{ spark_file_name }}.{{ spark.bin_type }} file and {{ spark_file_name }} Directory Already Exists"
  changed_when: spark_tarball_exists.stat.exists == True and spark_install_dir_exists.stat.exists == True


- name: Copy Apache Spark Tarball from local directory, roles/spark/files
  copy: src={{ spark_file_name }}.{{ spark.bin_type }} dest={{ spark.base_path }}/{{ spark_file_name }}.{{ spark.bin_type }} mode=0644 owner={{ hadoop.user }} group={{ hadoop.group }}
  register: local_copy_dss
  when: spark.download == false and spark_tarball_exists.stat.exists != True


- name: Unarchive Apache Spark Tarball
  unarchive:
    src: "{{ spark.base_path }}/{{ spark_file_name }}.{{ spark.bin_type }}"
    dest: "{{ spark.base_path }}"
    owner: "{{ spark.user }}"
    group: "{{ spark.group }}"
    mode: "0755"
    remote_src: yes
  register: unarchive_spark_tarball
  when: spark_install_dir_exists.stat.exists == false


- name: Install PySpark Pip Module
  shell: |
    su {{ hadoop.user }} -c 'pip install pyspark'

