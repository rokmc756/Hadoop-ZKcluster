## Apache Spark™ examples
This page shows you how to use different Apache Spark APIs with simple examples.
Spark is a great engine for small and large datasets. It can be used with single-node/localhost environments, or distributed clusters. Spark’s expansive API, excellent performance, and flexibility make it a good option for many analyses. This guide shows examples with the following Spark APIs:

- DataFrames
- SQL
- Structured Streaming
- RDDs

The examples use small datasets so the they are easy to follow.

## #xamples
* 01-dataframe.py
* 02-add-column.py
* 03-filter-dataframe.py
* 04-groupby-aggregatino-dataframe.py
* 05-sql-query-dataframe.py
* 06-persist-dataframe-parquet.py
* 07-structured-streaming.py
* 08-rdd-example.py


## Conclusion
These examples have shown how Spark provides nice user APIs for computations on small datasets. Spark can scale these same code examples to large datasets on distributed clusters. It’s fantastic how Spark can handle both large and small datasets.

Spark also has an expansive API compared with other query engines. Spark allows you to perform DataFrame operations with programmatic APIs, write SQL, perform streaming analyses, and do machine learning. Spark saves you from learning multiple frameworks and patching together various libraries to perform an analysis.


## Additional examples
Many additional examples are distributed with Spark:


### Basic Spark:
* Scala examples  - https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples
* Java examples   - https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples
* Python examples - https://github.com/apache/spark/tree/master/examples/src/main/python


### Spark Streaming
* Scala examples  - https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/streaming
* Java examples   - https://github.com/apache/spark/tree/master/examples/src/main/java/org/apache/spark/examples/streaming

