---
_config:
  base_path: "{{ _hadoop.base_path }}"
  path: "{{ _hadoop.base_path }}/{{ hive_file_name }}/conf"
  tmp: "{{ _hive.tmp_path }}"
  create_path:
    - "{{ _hive.tmp_path }}"
  warehouse: "{{ _hive.default_path }}/warehouse"
  scratchdir: "{{ _hive.default_path }}/tmp"
  querylog_location: "{{ _hive.default_path }}/log"
  logging_operation_log_location: "{{ _hive.tmp_path }}/{{ _hadoop.user }}/operation_logs"


_hdfs_path:
  - "{{ _config.warehouse }}"
  - "{{ _config.scratchdir }}"
  - "{{ _config.querylog_location }}"


# conn_host: "{{ hostvars[groups['database'][0]]['ansible_'~ netdev0]['ipv4']['address'] }}"
_db:
  type: "postgres"
  conn_driver_name: "org.postgresql.Driver"
  conn_host: "{{ hostvars[groups['database'][0]].ansible_ssh_host }}"
  conn_port: "5432"
  conn_dbname: "hive_testdb"
  conn_user_name: "hive_user"
  conn_password: "changeme"


_jdbc_connection_string: "jdbc:postgresql://{{ _db.conn_host }}:{{ _db.conn_port }}/{{ _db.conn_dbname }}?ssl=false"


_site_properties:
  - {
      "name":"hive.metastore.warehouse.dir",
      "value":"hdfs://{{ hostvars[groups['master'][0]].ansible_hostname }}:{{ _hadoop.hdfs_port }}{{ _config.warehouse }}"
  }
  - {
      "name":"hive.exec.scratchdir",
      "value":"{{ _config.scratchdir }}"
  }
  - {
      "name":"hive.querylog.location",
      "value":"{{ _config.querylog_location }}/hadoop"
  }
  - {
      "name":"javax.jdo.option.ConnectionURL",
      "value":"{{ _jdbc_connection_string }}"
  }
  - {
    "name":"javax.jdo.option.ConnectionDriverName",
    "value":"{{ _db.conn_driver_name }}"
  }
  - {
    "name":"javax.jdo.option.ConnectionUserName",
    "value":"{{ _db.conn_user_name }}"
  }
  - {
    "name":"javax.jdo.option.ConnectionPassword",
    "value":"{{ _db.conn_password }}"
  }
  - {
    "name":"hive.server2.logging.operation.log.location",
    "value":"{{ _config.logging_operation_log_location }}"
  }
  - {
    "name":"hive.exec.reducers.bytes.per.reducer",
    "value":"67108864"
  }


#  In order to avoid the issue Cannot connect to hive using beeline,
#  user root cannot impersonate anonymous when connecting to HiveServer2 from beeline,
#  from hadoop/etc/hadoop, add to the core-site.xml, with [username] is your local user that will be use in beeline
#
#  <property>
#    <name>hadoop.proxyuser.[username].groups</name>
#    <value>*</value>
#  </property>
#  <property>
#    <name>hadoop.proxyuser.[username].hosts</name>
#    <value>*</value>
#  </property>


# https://stackoverflow.com/questions/40339339/hive-query-does-not-begin-mapreduce-process-after-starting-job-and-generating-tr
# https://stackoverflow.com/questions/16203314/how-does-hive-choose-the-number-of-reducers-for-a-job
# https://stackoverflow.com/questions/30368437/reducers-for-hive-data
# https://stackoverflow.com/questions/61874740/insert-into-hive-using-ql-not-running
# https://github.com/chuqbach/Big-Data-Installation/blob/master/Hive%20Setup%20with%20Spark%20Exection%20and%20Spark%20HiveContext.md


_firewall_ports:
  - "{{ _hive.server_port }}"
  - "{{ _hive.hwi_port }}"
  - "{{ _hive.metastore_port }}"

